{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPyJMQoS3T3GHo0r+ScHzJ1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/araghavendra16/pytorch-intro/blob/main/DL_models_intro.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gIxe4CDVa87G",
        "outputId": "a87b769b-e21a-4d21-e267-fb7a5a55da23"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170498071/170498071 [==============================] - 2s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "94765736/94765736 [==============================] - 1s 0us/step\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet/mobilenet_1_0_224_tf_no_top.h5\n",
            "17225924/17225924 [==============================] - 0s 0us/step\n",
            "Training Basic CNN...\n",
            "Epoch 1/10\n",
            "625/625 - 88s - loss: 1.3461 - accuracy: 0.5204 - val_loss: 1.9470 - val_accuracy: 0.3804 - 88s/epoch - 140ms/step\n",
            "Epoch 2/10\n",
            "625/625 - 71s - loss: 0.9540 - accuracy: 0.6637 - val_loss: 1.2137 - val_accuracy: 0.5801 - 71s/epoch - 113ms/step\n",
            "Epoch 3/10\n",
            "625/625 - 71s - loss: 0.7591 - accuracy: 0.7348 - val_loss: 1.0938 - val_accuracy: 0.6312 - 71s/epoch - 113ms/step\n",
            "Epoch 4/10\n",
            "625/625 - 70s - loss: 0.6209 - accuracy: 0.7818 - val_loss: 0.9336 - val_accuracy: 0.6962 - 70s/epoch - 112ms/step\n",
            "Epoch 5/10\n",
            "625/625 - 72s - loss: 0.4998 - accuracy: 0.8234 - val_loss: 0.9737 - val_accuracy: 0.6891 - 72s/epoch - 115ms/step\n",
            "Epoch 6/10\n",
            "625/625 - 71s - loss: 0.3973 - accuracy: 0.8621 - val_loss: 1.0369 - val_accuracy: 0.6767 - 71s/epoch - 113ms/step\n",
            "Epoch 7/10\n",
            "625/625 - 74s - loss: 0.3031 - accuracy: 0.8943 - val_loss: 1.2752 - val_accuracy: 0.6679 - 74s/epoch - 119ms/step\n",
            "Epoch 8/10\n",
            "625/625 - 72s - loss: 0.2385 - accuracy: 0.9190 - val_loss: 1.4347 - val_accuracy: 0.6590 - 72s/epoch - 115ms/step\n",
            "Epoch 9/10\n",
            "625/625 - 71s - loss: 0.1942 - accuracy: 0.9323 - val_loss: 1.4789 - val_accuracy: 0.6664 - 71s/epoch - 113ms/step\n",
            "Epoch 10/10\n",
            "625/625 - 72s - loss: 0.1658 - accuracy: 0.9420 - val_loss: 1.4123 - val_accuracy: 0.6709 - 72s/epoch - 115ms/step\n",
            "Basic CNN Accuracy: 65.41%\n",
            "Training ResNet50...\n",
            "Epoch 1/10\n",
            "625/625 - 202s - loss: 2.1156 - accuracy: 0.2272 - val_loss: 2.0041 - val_accuracy: 0.2858 - 202s/epoch - 322ms/step\n",
            "Epoch 2/10\n",
            "625/625 - 199s - loss: 1.9663 - accuracy: 0.2901 - val_loss: 1.9534 - val_accuracy: 0.2781 - 199s/epoch - 318ms/step\n",
            "Epoch 3/10\n",
            "625/625 - 197s - loss: 1.9120 - accuracy: 0.3128 - val_loss: 1.8900 - val_accuracy: 0.3105 - 197s/epoch - 315ms/step\n",
            "Epoch 4/10\n",
            "625/625 - 198s - loss: 1.8745 - accuracy: 0.3291 - val_loss: 1.8269 - val_accuracy: 0.3590 - 198s/epoch - 317ms/step\n",
            "Epoch 5/10\n",
            "625/625 - 197s - loss: 1.8409 - accuracy: 0.3444 - val_loss: 1.8156 - val_accuracy: 0.3670 - 197s/epoch - 315ms/step\n",
            "Epoch 6/10\n",
            "625/625 - 197s - loss: 1.8249 - accuracy: 0.3511 - val_loss: 1.8276 - val_accuracy: 0.3342 - 197s/epoch - 316ms/step\n",
            "Epoch 7/10\n",
            "625/625 - 195s - loss: 1.8087 - accuracy: 0.3577 - val_loss: 1.8201 - val_accuracy: 0.3574 - 195s/epoch - 312ms/step\n",
            "Epoch 8/10\n",
            "625/625 - 198s - loss: 1.7919 - accuracy: 0.3621 - val_loss: 1.7705 - val_accuracy: 0.3777 - 198s/epoch - 316ms/step\n",
            "Epoch 9/10\n",
            "625/625 - 199s - loss: 1.7798 - accuracy: 0.3691 - val_loss: 1.7763 - val_accuracy: 0.3728 - 199s/epoch - 318ms/step\n",
            "Epoch 10/10\n",
            "625/625 - 198s - loss: 1.7688 - accuracy: 0.3710 - val_loss: 1.7593 - val_accuracy: 0.3812 - 198s/epoch - 317ms/step\n",
            "ResNet50 Accuracy: 37.94%\n",
            "Training MobileNet...\n",
            "Epoch 1/10\n",
            "625/625 - 45s - loss: 2.2438 - accuracy: 0.1700 - val_loss: 2.2074 - val_accuracy: 0.1992 - 45s/epoch - 72ms/step\n",
            "Epoch 2/10\n",
            "625/625 - 42s - loss: 2.1947 - accuracy: 0.2045 - val_loss: 2.1817 - val_accuracy: 0.2154 - 42s/epoch - 67ms/step\n",
            "Epoch 3/10\n",
            "625/625 - 41s - loss: 2.1777 - accuracy: 0.2125 - val_loss: 2.1699 - val_accuracy: 0.2177 - 41s/epoch - 65ms/step\n",
            "Epoch 4/10\n",
            "625/625 - 40s - loss: 2.1688 - accuracy: 0.2169 - val_loss: 2.1638 - val_accuracy: 0.2221 - 40s/epoch - 64ms/step\n",
            "Epoch 5/10\n",
            "625/625 - 42s - loss: 2.1630 - accuracy: 0.2201 - val_loss: 2.1595 - val_accuracy: 0.2204 - 42s/epoch - 67ms/step\n",
            "Epoch 6/10\n",
            "625/625 - 40s - loss: 2.1590 - accuracy: 0.2211 - val_loss: 2.1560 - val_accuracy: 0.2218 - 40s/epoch - 64ms/step\n",
            "Epoch 7/10\n",
            "625/625 - 42s - loss: 2.1561 - accuracy: 0.2207 - val_loss: 2.1539 - val_accuracy: 0.2229 - 42s/epoch - 67ms/step\n",
            "Epoch 8/10\n",
            "625/625 - 42s - loss: 2.1540 - accuracy: 0.2224 - val_loss: 2.1525 - val_accuracy: 0.2247 - 42s/epoch - 67ms/step\n",
            "Epoch 9/10\n",
            "625/625 - 42s - loss: 2.1523 - accuracy: 0.2221 - val_loss: 2.1515 - val_accuracy: 0.2243 - 42s/epoch - 67ms/step\n",
            "Epoch 10/10\n",
            "625/625 - 40s - loss: 2.1508 - accuracy: 0.2231 - val_loss: 2.1514 - val_accuracy: 0.2250 - 40s/epoch - 64ms/step\n",
            "MobileNet Accuracy: 22.13%\n",
            "Basic CNN Test Accuracy: 65.41%\n",
            "ResNet50 Test Accuracy: 37.94%\n",
            "MobileNet Test Accuracy: 22.13%\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.applications import ResNet50, MobileNet\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Load CIFAR-10 data\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "# Normalize data\n",
        "x_train = x_train.astype('float32') / 255.0\n",
        "x_test = x_test.astype('float32') / 255.0\n",
        "\n",
        "# Convert labels to one-hot encoding\n",
        "y_train = to_categorical(y_train, 10)\n",
        "y_test = to_categorical(y_test, 10)\n",
        "\n",
        "# Define the basic CNN model\n",
        "# Basic CNN Model:\n",
        "# - This model is a straightforward example of a Convolutional Neural Network.\n",
        "# - It consists of multiple convolutional layers with ReLU activation, followed by max-pooling layers.\n",
        "# - Batch normalization is applied after each convolution operation to help stabilize learning and reduce the number of training epochs required for convergence.\n",
        "# - The network ends with fully connected layers that classify the images into one of the ten categories based on the learned features.\n",
        "# - This model is good for understanding the basic building blocks of CNNs and is quite efficient on small to medium-sized datasets.\n",
        "def basic_cnn():\n",
        "    model = Sequential([\n",
        "        Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)),\n",
        "        BatchNormalization(),\n",
        "        MaxPooling2D((2, 2)),\n",
        "        Conv2D(64, (3, 3), activation='relu'),\n",
        "        BatchNormalization(),\n",
        "        MaxPooling2D((2, 2)),\n",
        "        Conv2D(128, (3, 3), activation='relu'),\n",
        "        Flatten(),\n",
        "        Dense(128, activation='relu'),\n",
        "        Dense(10, activation='softmax')\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "# Define the ResNet model\n",
        "# ResNet50 Model using Transfer Learning:\n",
        "# - ResNet, or Residual Network, introduces a novel architecture with \"skip connections\" or \"residual blocks\" allowing the network to skip one or more layers.\n",
        "# - These connections help combat the vanishing gradient problem in deep neural networks, enabling training of very deep networks without performance degradation.\n",
        "# - Here, we use ResNet50, a variant with 50 layers, pre-trained on ImageNet, employing it for transfer learning.\n",
        "# - The model is customized by adding a new top layer (a fully connected layer) to classify images into the CIFAR-10 classes.\n",
        "# - The convolution base is frozen during training, meaning only the top layer's weights are updated. This is efficient when the new dataset is similar to the dataset on which the model was originally trained.\n",
        "\n",
        "def resnet_model():\n",
        "    base_model = ResNet50(include_top=False, weights='imagenet', input_shape=(32, 32, 3))\n",
        "    base_model.trainable = False\n",
        "    model = Sequential([\n",
        "        base_model,\n",
        "        Flatten(),\n",
        "        Dense(10, activation='softmax')\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "# Define the MobileNet model\n",
        "# MobileNet Model using Transfer Learning:\n",
        "# - MobileNet is designed specifically for mobile and embedded devices with constraints on memory and computational power.\n",
        "# - It utilizes depthwise separable convolutions which significantly reduce the number of parameters compared to a standard convolution with similar performance.\n",
        "# - Like ResNet50, this model uses a pre-trained MobileNet model with a customized top layer for CIFAR-10 classification.\n",
        "# - The lightweight nature of MobileNet makes it ideal for applications requiring high efficiency and speed, such as real-time image recognition on mobile devices.\n",
        "# - As with ResNet50, transfer learning is applied here, updating only the top layer weights during training to adapt to the CIFAR-10 dataset.\n",
        "\n",
        "def mobilenet_model():\n",
        "    base_model = MobileNet(include_top=False, weights='imagenet', input_shape=(32, 32, 3))\n",
        "    base_model.trainable = False\n",
        "    model = Sequential([\n",
        "        base_model,\n",
        "        Flatten(),\n",
        "        Dense(10, activation='softmax')\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "# Compile and train models\n",
        "models = {'Basic CNN': basic_cnn(), 'ResNet50': resnet_model(), 'MobileNet': mobilenet_model()}\n",
        "results = {}\n",
        "\n",
        "for name, model in models.items():\n",
        "    print(f\"Training {name}...\")\n",
        "    model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    history = model.fit(x_train, y_train, epochs=10, batch_size=64, validation_split=0.2, verbose=2)\n",
        "    results[name] = model.evaluate(x_test, y_test, verbose=0)\n",
        "    print(f\"{name} Accuracy: {results[name][1] * 100:.2f}%\")\n",
        "\n",
        "# Compare the performance\n",
        "for name, result in results.items():\n",
        "    print(f\"{name} Test Accuracy: {result[1] * 100:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Basic CNN Test Accuracy: 65.41%\n",
        "This model was trained from scratch on CIFAR-10, benefiting from the simplicity and direct applicability to the task at hand.\n",
        "Overfitting: There is some evidence of overfitting, we observe training loss decreasing and the validation loss starting to increase. This is typical for models that learn to fit the training data too well, at the expense of generalizing to new data.\n",
        "\n",
        "\n",
        "ResNet50 Test Accuracy: 37.94%\n",
        "The warning (input_shape is undefined or non-square, or rows is not in [128, 160, 192, 224]) suggests that the pre-trained model did not adapt well to the CIFAR-10 images' size of 32x32. ResNet50 is typically used with larger images (like 224x224), and using it directly on smaller images without appropriate adjustments or pre-processing can lead to poor performance.\n",
        "The model was pre-trained on ImageNet and then directly applied to CIFAR-10 without fine-tuning the deeper layers (which contain more abstract representations) likely contributed to its underperformance.\n",
        "\n",
        "\n",
        "MobileNet Test Accuracy: 22.13%\n",
        "Like ResNet50, MobileNet is designed for input images of size 224x224. The small size of CIFAR-10 images (32x32) means that the model may not capture enough features effectively, resulting in poor performance.\n",
        "The use of MobileNet with its parameters frozen (except for the top layer) also limits its ability to learn CIFAR-10 specific features effectively.\n",
        "\n",
        "Ideas:\n",
        "In models built from scratch, like the Basic CNN, no layers are frozen because the model needs to learn all the features from the beginning specifically for the given task.\n",
        "\n",
        "Transfer Learning Models: In pre-trained models like ResNet50 and MobileNet used in our experiment, the convolutional layers are typically frozen during the initial phase of training. This approach is taken because these layers contain useful features learned on ImageNet, which are likely beneficial for the new task.\n",
        "\n",
        "The idea behind freezing the layers is to preserve the features that the model has already learned from a large and diverse dataset like ImageNet. These features are generally quite versatile, ranging from simple edge detectors in the early layers to complex object detectors in the deeper layers.\n"
      ],
      "metadata": {
        "id": "toZp5dKrvAcQ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0YACFQVhvEzT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}